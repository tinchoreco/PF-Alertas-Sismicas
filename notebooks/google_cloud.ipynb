{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "#from datetime import datetime, timedelta\n",
    "\n",
    "#from google.api_core.exceptions import Conflict\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Creamos los permisos con la llave para ingresar a Google \n",
    "# activamos el servicio\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../notebooks/Clave_Google/henry-sismos-a343182ba163.json\"\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos la ETL por Pais para google cloud\n",
    "\n",
    "Ya teniendo la conexión y un código funcional, es el momento de aplicar la ETL ya desarrollada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colombia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n",
      "La tabla Colombia_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['CO']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_colombia_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['CO']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla Japan_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '28.691',\n",
    "    'maxlatitude': '47.458',\n",
    "    'minlongitude': '125.859',\n",
    "    'maxlongitude': '156.445',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['JP']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Japan_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_japan_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Japan_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '28.691',\n",
    "            'maxlatitude': '47.458',\n",
    "            'minlongitude': '125.859',\n",
    "            'maxlongitude': '156.445',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['JP']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla EEUU_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '12.555',\n",
    "    'maxlatitude': '75.141',\n",
    "    'minlongitude': '182.813',\n",
    "    'maxlongitude': '315',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['US']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'EEUU_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_EEUU_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'EEUU_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '12.555',\n",
    "            'maxlatitude': '75.141',\n",
    "            'minlongitude': '182.813',\n",
    "            'maxlongitude': '315',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['US']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
